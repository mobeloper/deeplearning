{"cells":[{"cell_type":"markdown","metadata":{"id":"D4CNzLHQWbPg"},"source":["#__Video Classification Using Hybrid Model__\n","Let's see how to classify the video using transfer learning and a recurrent model on the UCF101 dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"ktbWX3ddWxa9"},"source":["## Steps to Be Followed:\n","1. Downloading data and importing the required libraries\n","2. Reading the data from datasets and printing the ten rows\n","3. Defining the functions for cropping and loading video frames\n","4. Building a feature extraction model using InceptionV3 architecture\n","5. Creating a string lookup table for labels and printing the vocabulary of the label processor\n","6. Preparing video data for training and testing by extracting frame features\n","7. Defining and training a sequence model using GRU layers\n","8. Loading a test video, extracting frame features, and making predictions using the sequence model"]},{"cell_type":"markdown","metadata":{"id":"YvjlPz3dW5Bs"},"source":["### Step 1: Downloading Data and Importing the Required Libraries\n","- Download the dataset\n","- Import the required libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0t8MHucxPGBU"},"outputs":[{"name":"stdout","output_type":"stream","text":["zsh:1: command not found: wget\n","zsh:1: command not found: wget\n"]}],"source":["!wget -q --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\n","!wget -q --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FufAi4ZnPJ7S"},"outputs":[],"source":["%%capture\n","!unrar e UCF101.rar data/\n","!unzip -qq UCF101TrainTestSplits-RecognitionTask.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DT8IxSDPJ-U"},"outputs":[],"source":["from tensorflow import keras\n","from imutils import paths\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import imageio\n","import cv2\n","import os\n","import shutil"]},{"cell_type":"markdown","metadata":{"id":"Zdj2UXjiFr0h"},"source":["Open the __.txt__ file which has the names of the training videos\n","\n","Create a dataframe having video names\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"BoFiu5HpPKBt","outputId":"152a27a1-a46e-494f-8fd6-c409d0292980"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-ab18e250-c6a6-4266-9b50-51c7ccb74a7b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab18e250-c6a6-4266-9b50-51c7ccb74a7b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ab18e250-c6a6-4266-9b50-51c7ccb74a7b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ab18e250-c6a6-4266-9b50-51c7ccb74a7b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                      video_name\n","0  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n","1  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1\n","2  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1\n","3  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1\n","4  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["\n","f = open(\"ucfTrainTestlist/trainlist01.txt\", \"r\")\n","temp = f.read()\n","videos = temp.split('\\n')\n","\n","train = pd.DataFrame()\n","train['video_name'] = videos\n","train = train[:-1]\n","train.head()"]},{"cell_type":"markdown","metadata":{"id":"1F6_vxikF3zd"},"source":["Open the __.txt__ file which has the names of the test videos\n","\n","Create a DataFrame having video names"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"fGH6nO23PKFO","outputId":"dc024c6c-54f9-4d0a-d839-0e0a5bd5ccbf"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-efcb607c-294a-41d8-b960-d574c4932197\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efcb607c-294a-41d8-b960-d574c4932197')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-efcb607c-294a-41d8-b960-d574c4932197 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-efcb607c-294a-41d8-b960-d574c4932197');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                    video_name\n","0  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\n","1  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi\n","2  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi\n","3  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi\n","4  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["\n","with open(\"ucfTrainTestlist/testlist01.txt\", \"r\") as f:\n","    temp = f.read()\n","videos = temp.split(\"\\n\")\n","\n","test = pd.DataFrame()\n","test[\"video_name\"] = videos\n","test = test[:-1]\n","test.head()"]},{"cell_type":"markdown","metadata":{"id":"mwJeWtfLHQs-"},"source":["- Define the __extract_tag__ function that extracts a tag from the video path. This is done by splitting the video path by or and returning the first part.\n","- Define the __separate_video_name function, which separates the video name from the video path. This is achieved by splitting the video name by / and returning the second part.\n","- Define the __rectify_video_name__ function to rectify the video name by splitting the video name by \" \" and returning the first part.\n","- Define the __move_videos__ function:\n","   - Check if the output directory exists. If not, create the directory using __os.mkdir__.\n","   - Iterate over the DataFrame, __df__, using a progress bar from the __tqdm__ library.\n","   - For each row in the DataFrame, extract the video file name from the __video_name__ column, create its path, and then copy the video file to the output directory using __shutil.copy2__.\n","   - After the loop ends, print the total number of videos in the output directory.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yi9ooC9XPfYe"},"outputs":[],"source":["def extract_tag(video_path):\n","    return video_path.split(\"/\")[0]\n","\n","def separate_video_name(video_name):\n","    return video_name.split(\"/\")[1]\n","\n","def rectify_video_name(video_name):\n","    return video_name.split(\" \")[0]\n","\n","def move_videos(df, output_dir):\n","    if not os.path.exists(output_dir):\n","        os.mkdir(output_dir)\n","    for i in tqdm(range(df.shape[0])):\n","        videoFile = df['video_name'][i].split(\"/\")[-1]\n","        videoPath = os.path.join(\"data\", videoFile)\n","        shutil.copy2(videoPath, output_dir)\n","    print()\n","    print(f\"Total videos: {len(os.listdir(output_dir))}\")"]},{"cell_type":"markdown","metadata":{"id":"55J4781CXO9b"},"source":["### Step 2: Reading the Data from Datasets and Printing the Ten Rows\n","\n","- Define the values of **IMG_SIZE**, **BATCH_SIZE**, **EPOCHS**, **MAX_SEQ_LENGTH**, and **NUM_FEATURES**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwjxIY0xPlw7"},"outputs":[],"source":["IMG_SIZE = 224\n","BATCH_SIZE = 64\n","EPOCHS = 2\n","\n","MAX_SEQ_LENGTH = 20\n","NUM_FEATURES = 2048"]},{"cell_type":"markdown","metadata":{"id":"IUlUP1OOXWc4"},"source":["### Step 3: Defining the Functions for Cropping and Loading Video Frames\n","- Define a function named **crop_center_square** that crops a frame to a square shape by determining the minimum dimension and calculating the starting coordinates\n","- Define a function named __load_video__ that loads a video file, crops each frame to a square shape, resizes it, and converts the color channels.\n","- Open the video file using **cv2.VideoCapture** and initialize an empty list called frames\n","- Read frames from the video, crop them to a square shape, resize them, convert the color channels, and append them to the frames list\n","- If the maximum number of frames is reached or the video ends, exit the loop.\n","- Release the video capture.\n","- Convert the frame list to a NumPy array\n","- Return the array of frames"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HzfusTJPpPV"},"outputs":[],"source":["def crop_center_square(frame):\n","    y, x = frame.shape[0:2]\n","    min_dim = min(y, x)\n","    start_x = (x // 2) - (min_dim // 2)\n","    start_y = (y // 2) - (min_dim // 2)\n","    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n","\n","\n","def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n","    cap = cv2.VideoCapture(path)\n","    frames = []\n","    try:\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = crop_center_square(frame)\n","            frame = cv2.resize(frame, resize)\n","            frame = frame[:, :, [2, 1, 0]]\n","            frames.append(frame)\n","\n","            if len(frames) == max_frames:\n","                break\n","    finally:\n","        cap.release()\n","    return np.array(frames)"]},{"cell_type":"markdown","metadata":{"id":"2j2lXMm4Xd8V"},"source":["__Observation:__\n","- The code defines two functions, **crop_center_square** and __load_video__, which can be used to crop frames from videos and load videos as arrays of frames, respectively."]},{"cell_type":"markdown","metadata":{"id":"14OJjHbUXjrh"},"source":["### Step 4: Building a Feature Extraction Model Using InceptionV3 Architecture\n","\n","- Create a feature extractor using the InceptionV3 model from keras.applications with specific configurations.\n","- Assign the preprocess_input function from **keras.applications.inception_v3** to the v**ariable preprocess_input**.\n","- Create an input layer with the shape __(IMG_SIZE, IMG_SIZE, 3)__ using **keras.Input**.\n","- Preprocess the input using the **preprocess_input function**.\n","- Pass the preprocessed input through the feature extractor to obtain the outputs.\n","- Create a model with the inputs and outputs using **keras.Model** and assign it to the variable **feature_extractor**."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rjl2yoPmPtoW","outputId":"bcaab3cb-6a49-4918-f7d4-d3986be9ea6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","87910968/87910968 [==============================] - 4s 0us/step\n"]}],"source":["def build_feature_extractor():\n","    \n","    #Use a pre-trained CNN model (inceptionV3)\n","    feature_extractor = keras.applications.InceptionV3(\n","        weights=\"imagenet\",\n","        include_top=False,\n","        pooling=\"avg\",\n","        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n","    )\n","    \n","    #pre-process input data\n","    preprocess_input = keras.applications.inception_v3.preprocess_input\n","\n","    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n","    preprocessed = preprocess_input(inputs)\n","\n","    outputs = feature_extractor(preprocessed)\n","    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n","\n","\n","# Make the CNN model\n","feature_extractor = build_feature_extractor()"]},{"cell_type":"markdown","metadata":{"id":"9rN_SBAOYG8B"},"source":["__Observations:__\n","- The code defines a function **build_feature_extractor** that creates a feature extractor model using InceptionV3 architecture.\n","- The model takes inputs of size __(IMG_SIZE, IMG_SIZE, 3)__, preprocesses the inputs, and produces the outputs.\n","- The created feature extractor model is assigned to the variable **feature_extractor**."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"XMkKyQQ6QCMV","outputId":"3b56b986-6795-43fe-a6e4-b59969ae255e"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-d9a8dc71-592b-4ffd-a8d2-cecfc1edc018\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_name</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>v_ApplyEyeMakeup_g08_c01.avi 1</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>v_ApplyEyeMakeup_g08_c02.avi 1</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>v_ApplyEyeMakeup_g08_c03.avi 1</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>v_ApplyEyeMakeup_g08_c04.avi 1</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>v_ApplyEyeMakeup_g08_c05.avi 1</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9a8dc71-592b-4ffd-a8d2-cecfc1edc018')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d9a8dc71-592b-4ffd-a8d2-cecfc1edc018 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d9a8dc71-592b-4ffd-a8d2-cecfc1edc018');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                       video_name             tag\n","0  v_ApplyEyeMakeup_g08_c01.avi 1  ApplyEyeMakeup\n","1  v_ApplyEyeMakeup_g08_c02.avi 1  ApplyEyeMakeup\n","2  v_ApplyEyeMakeup_g08_c03.avi 1  ApplyEyeMakeup\n","3  v_ApplyEyeMakeup_g08_c04.avi 1  ApplyEyeMakeup\n","4  v_ApplyEyeMakeup_g08_c05.avi 1  ApplyEyeMakeup"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train[\"tag\"] = train[\"video_name\"].apply(extract_tag)\n","train[\"video_name\"] = train[\"video_name\"].apply(separate_video_name)\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"ialV1htPQGlg","outputId":"30c424d1-bd7b-44fd-f838-e3838385fda9"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-b7a0e0e7-6213-4650-9226-bc7030dcfa93\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_name</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>v_ApplyEyeMakeup_g08_c01.avi</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>v_ApplyEyeMakeup_g08_c02.avi</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>v_ApplyEyeMakeup_g08_c03.avi</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>v_ApplyEyeMakeup_g08_c04.avi</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>v_ApplyEyeMakeup_g08_c05.avi</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7a0e0e7-6213-4650-9226-bc7030dcfa93')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b7a0e0e7-6213-4650-9226-bc7030dcfa93 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b7a0e0e7-6213-4650-9226-bc7030dcfa93');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                     video_name             tag\n","0  v_ApplyEyeMakeup_g08_c01.avi  ApplyEyeMakeup\n","1  v_ApplyEyeMakeup_g08_c02.avi  ApplyEyeMakeup\n","2  v_ApplyEyeMakeup_g08_c03.avi  ApplyEyeMakeup\n","3  v_ApplyEyeMakeup_g08_c04.avi  ApplyEyeMakeup\n","4  v_ApplyEyeMakeup_g08_c05.avi  ApplyEyeMakeup"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train[\"video_name\"] = train[\"video_name\"].apply(rectify_video_name)\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"a07ssIhrQL_H","outputId":"4c172777-c48e-4492-d297-e36644594e9e"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-1a986ca2-5c9c-45b9-ab6c-8e7997bd9553\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_name</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>v_ApplyEyeMakeup_g01_c01.avi</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>v_ApplyEyeMakeup_g01_c02.avi</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>v_ApplyEyeMakeup_g01_c03.avi</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>v_ApplyEyeMakeup_g01_c04.avi</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>v_ApplyEyeMakeup_g01_c05.avi</td>\n","      <td>ApplyEyeMakeup</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a986ca2-5c9c-45b9-ab6c-8e7997bd9553')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1a986ca2-5c9c-45b9-ab6c-8e7997bd9553 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1a986ca2-5c9c-45b9-ab6c-8e7997bd9553');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                     video_name             tag\n","0  v_ApplyEyeMakeup_g01_c01.avi  ApplyEyeMakeup\n","1  v_ApplyEyeMakeup_g01_c02.avi  ApplyEyeMakeup\n","2  v_ApplyEyeMakeup_g01_c03.avi  ApplyEyeMakeup\n","3  v_ApplyEyeMakeup_g01_c04.avi  ApplyEyeMakeup\n","4  v_ApplyEyeMakeup_g01_c05.avi  ApplyEyeMakeup"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["test[\"tag\"] = test[\"video_name\"].apply(extract_tag)\n","test[\"video_name\"] = test[\"video_name\"].apply(separate_video_name)\n","test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6zxubqKQQav","outputId":"528512c0-4ad4-4973-853c-8161fd352294"},"outputs":[{"data":{"text/plain":["((1171, 2), (459, 2))"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["n = 10\n","topNActs = train[\"tag\"].value_counts().nlargest(n).reset_index()[\"index\"].tolist()\n","train_new = train[train[\"tag\"].isin(topNActs)]\n","test_new = test[test[\"tag\"].isin(topNActs)]\n","train_new.shape, test_new.shape"]},{"cell_type":"markdown","metadata":{"id":"-Q2oQQC5KQZc"},"source":["**Observation:**\n","- The output **((1171, 2), (459, 2))** is a tuple showing the shapes of **train_new** and **test_new**. The **train_new** DataFrame has 1171 rows and 2 columns, and the **test_new** DataFrame has 459 rows and 2 columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBPvTP0RQUGb"},"outputs":[],"source":["train_new = train_new.reset_index(drop=True)\n","test_new = test_new.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"tNw7O0LdXzFV"},"source":["### Step 5: Creating a String Lookup Table for Labels and Prints the Vocabulary of the Label Processor\n","- Create a label processor using **keras.layers.StringLookup**\n","- Set the number of out-of-vocabulary (OOV) indices to **0**\n","- Set the vocabulary of the label processor to the unique values from the **tag** column of the **train_df** DataFrame\n","- Retrieve the vocabulary of the label processor using **label_processor.get_vocabulary()**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tHg5adMbTlyr","outputId":"aa40c69a-6994-4081-f3eb-e7d427dda0ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', 'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', 'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'HammerThrow', 'Hammering', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', 'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', 'JugglingBalls', 'JumpRope', 'JumpingJack', 'Kayaking', 'Knitting', 'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', 'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', 'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', 'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', 'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', 'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', 'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', 'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', 'YoYo']\n"]}],"source":["label_processor = keras.layers.StringLookup(\n","    num_oov_indices=0, vocabulary=np.unique(train[\"tag\"])\n",")\n","print(label_processor.get_vocabulary())"]},{"cell_type":"markdown","metadata":{"id":"KF_te3XFYQku"},"source":["__Observations:__\n","- The code creates a label processor that maps labels from text to integer indices.\n","- It uses the unique values from the **tag** column of the **train_df** DataFrame as the vocabulary for the label processor.\n","- The output is the vocabulary of the label processor, which is a list of unique labels."]},{"cell_type":"markdown","metadata":{"id":"xsh5ULu7YU36"},"source":["### Step 6: Preparing Video Data for Training and Testing by Extracting Frame Features\n","- Define a function named **prepare_all_videos** that inputs a DataFrame (df) and a root directory **(root_dir)**.\n","- Retrieve the video paths and labels from the DataFrame and encode the labels using **label_processor**.\n","- Initialize arrays to store frame masks and frame features for each video.\n","- Iterate over each video in the dataset, load the frames, and extract features using the **feature_extractor** model.\n","- Update the arrays with the extracted features and masks for each video.\n","- Call the **prepare_all_videos** function on the train and test DataFrames, storing the returned values in **train_data**, **train_labels**, **test_data**, and __test_labels__.\n","- Finally, print the shape of the frame features in the train set and the shape of the frame masks in the train set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ARvHX2VATt36","outputId":"119739be-20c0-4685-b11d-3f5314a09d14"},"outputs":[{"name":"stdout","output_type":"stream","text":["Frame features in train set: (9537, 20, 2048)\n","Frame masks in train set: (9537, 20)\n"]}],"source":["def prepare_all_videos(df, root_dir):\n","    num_samples = len(df)\n","    video_paths = df[\"video_name\"].values.tolist()\n","    labels = df[\"tag\"].values\n","    labels = label_processor(labels[..., None]).numpy()\n","\n","    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n","    frame_features = np.zeros(\n","        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n","    )\n","\n","    for idx, path in enumerate(video_paths):\n","        frames = load_video(os.path.join(root_dir, path))\n","        frames = frames[None, ...]\n","\n","        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n","        temp_frame_features = np.zeros(\n","            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n","        )\n","\n","        for i, batch in enumerate(frames):\n","            video_length = batch.shape[0]\n","            length = min(MAX_SEQ_LENGTH, video_length)\n","            for j in range(length):\n","                temp_frame_features[i, j, :] = feature_extractor.predict(\n","                    batch[None, j, :]\n","                )\n","            temp_frame_mask[i, :length] = 1\n","\n","        frame_features[idx,] = temp_frame_features.squeeze()\n","        frame_masks[idx,] = temp_frame_mask.squeeze()\n","\n","    return (frame_features, frame_masks), labels\n","\n","\n","train_data, train_labels = prepare_all_videos(train, \"train\")\n","test_data, test_labels = prepare_all_videos(test, \"test\")\n","\n","print(f\"Frame features in train set: {train_data[0].shape}\")\n","print(f\"Frame masks in train set: {train_data[1].shape}\")"]},{"cell_type":"markdown","metadata":{"id":"ohCZ1_qtYa8e"},"source":["__Observations:__\n","- The code processes videos by extracting frame features and creating frame masks.\n","- It then returns the frame features, frame masks, and labels for the train and test sets.\n","- The output is the shape of the frame features in the train set and the shape of the frame masks in the train set."]},{"cell_type":"markdown","metadata":{"id":"ztRcM2knYgPA"},"source":["### Step 7: Defining and Training a Sequence Model Using GRU Layers\n","- Define a function named **get_sequence_model** that creates a sequence model for video classification\n","- Create input layers for frame features and masks\n","- Apply two GRU layers to the frame features input, with the second GRU layer returning only the last output\n","- Add a dropout layer, a dense layer with ReLU activation, and a final dense layer with softmax activation for the output\n","- Compile the model with sparse categorical cross-entropy loss, Adam optimizer, and accuracy metric\n","- Define a function named __run_experiment__ for running the training and evaluation\n","- Set up a checkpoint to save the best model during training\n","- Create the sequence model using **get_sequence_model**\n","- Train the model on the training data with a validation split, specified number of epochs, and the checkpoint callback\n","- Load the best weights saved during training\n","- Evaluate the model on the test data and print the test accuracy\n","- Return the history object and the trained sequence model\n","- Call the **run_experiment** function and store the returned values in __(history)__ and **sequence_model**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KAgMUDWKT1Ba","outputId":"b5b947ee-ec4e-41a1-e021-98323346a3a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","208/209 [============================>.] - ETA: 0s - loss: 4.5801 - accuracy: 0.0164\n","Epoch 1: val_loss improved from inf to 4.78500, saving model to /tmp/video_classifier\n","209/209 [==============================] - 24s 68ms/step - loss: 4.5800 - accuracy: 0.0163 - val_loss: 4.7850 - val_accuracy: 0.0000e+00\n","Epoch 2/2\n","209/209 [==============================] - ETA: 0s - loss: 4.5185 - accuracy: 0.0162\n","Epoch 2: val_loss did not improve from 4.78500\n","209/209 [==============================] - 11s 53ms/step - loss: 4.5185 - accuracy: 0.0162 - val_loss: 4.9506 - val_accuracy: 0.0000e+00\n","119/119 [==============================] - 2s 14ms/step - loss: 4.6181 - accuracy: 0.0130\n","Test accuracy: 1.3%\n"]}],"source":["def get_sequence_model():\n","    class_vocab = label_processor.get_vocabulary()\n","\n","    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n","    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n","\n","    x = keras.layers.GRU(16, return_sequences=True)(\n","        frame_features_input, mask=mask_input\n","    )\n","    x = keras.layers.GRU(8)(x)\n","    x = keras.layers.Dropout(0.4)(x)\n","    x = keras.layers.Dense(8, activation=\"relu\")(x)\n","    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n","\n","    rnn_model = keras.Model([frame_features_input, mask_input], output)\n","\n","    rnn_model.compile(\n","        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n","    )\n","    return rnn_model\n","\n","\n","def run_experiment():\n","    filepath = \"/tmp/video_classifier\"\n","    checkpoint = keras.callbacks.ModelCheckpoint(\n","        filepath, save_weights_only=True, save_best_only=True, verbose=1\n","    )\n","\n","    seq_model = get_sequence_model()\n","    history = seq_model.fit(\n","        [train_data[0], train_data[1]],\n","        train_labels,\n","        validation_split=0.3,\n","        epochs=EPOCHS,\n","        callbacks=[checkpoint],\n","    )\n","\n","    seq_model.load_weights(filepath)\n","    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","\n","    return history, seq_model\n","\n","\n","_, sequence_model = run_experiment()"]},{"cell_type":"markdown","metadata":{"id":"MdIBd6N_Ym1z"},"source":["__Observations:__\n","- Training progress and validation metrics will be displayed during the model training process.\n","- After training, the model will be evaluated on the test data, and the test accuracy will be printed."]},{"cell_type":"markdown","metadata":{"id":"RZShMzB7Yyqp"},"source":["### Step 8: Loading a Test Video, Extracting Frame Features, and Making Predictions Using the Sequence Model\n","- Load a random test video path.\n","- Call the **sequence_prediction** function with the test video path.\n","- Within the **sequence_prediction** function:\n","\n","  a. Get the vocabulary of the classes.\n","\n","  b. Load the frames of the video.\n","\n","  c. Prepare the frames for sequence prediction by extracting frame features and creating a frame mask.\n","\n","  d. Use the trained sequence model to predict the probabilities of each class for the video.\n","\n","  e. Print the predicted class probabilities in descending order.\n","\n","- Assign the frames of the test video to the variable test_frames."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xezv1Nq3UDoz","outputId":"5b213d4e-9495-4b51-b7b5-28ebf166424b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test video path: v_TaiChi_g05_c03.avi\n","1/1 [==============================] - 4s 4s/step\n","  PlayingGuitar:  1.09%\n","  PlayingCello:  1.09%\n","  Drumming:  1.09%\n","  Billiards:  1.09%\n","  BenchPress:  1.09%\n","  PlayingDaf:  1.09%\n","  HorseRiding:  1.09%\n","  CricketShot:  1.09%\n","  Bowling:  1.08%\n","  PlayingDhol:  1.08%\n","  IceDancing:  1.08%\n","  PlayingFlute:  1.08%\n","  BandMarching:  1.08%\n","  BaseballPitch:  1.07%\n","  PoleVault:  1.07%\n","  PlayingSitar:  1.07%\n","  HammerThrow:  1.07%\n","  Kayaking:  1.07%\n","  BoxingPunchingBag:  1.07%\n","  Hammering:  1.07%\n","  CliffDiving:  1.07%\n","  JumpRope:  1.07%\n","  Basketball:  1.07%\n","  Nunchucks:  1.07%\n","  ApplyEyeMakeup:  1.07%\n","  Archery:  1.07%\n","  Diving:  1.07%\n","  HandstandPushups:  1.07%\n","  CricketBowling:  1.07%\n","  FrontCrawl:  1.06%\n","  HeadMassage:  1.06%\n","  BabyCrawling:  1.06%\n","  GolfSwing:  1.06%\n","  BasketballDunk:  1.06%\n","  MilitaryParade:  1.06%\n","  LongJump:  1.06%\n","  BrushingTeeth:  1.06%\n","  Biking:  1.06%\n","  BoxingSpeedBag:  1.06%\n","  BlowDryHair:  1.06%\n","  Haircut:  1.06%\n","  HulaHoop:  1.06%\n","  HorseRace:  1.05%\n","  FloorGymnastics:  1.05%\n","  JumpingJack:  1.05%\n","  Lunges:  1.05%\n","  HighJump:  1.05%\n","  FieldHockeyPenalty:  1.05%\n","  Mixing:  1.05%\n","  Knitting:  1.05%\n","  PommelHorse:  1.05%\n","  JavelinThrow:  1.04%\n","  FrisbeeCatch:  1.04%\n","  CleanAndJerk:  1.04%\n","  MoppingFloor:  1.04%\n","  ApplyLipstick:  1.04%\n","  CuttingInKitchen:  1.04%\n","  Fencing:  1.04%\n","  ParallelBars:  1.04%\n","  PizzaTossing:  1.04%\n","  BlowingCandles:  1.04%\n","  BodyWeightSquats:  1.04%\n","  HandstandWalking:  1.04%\n","  PlayingTabla:  1.03%\n","  JugglingBalls:  1.03%\n","  BreastStroke:  1.03%\n","  BalanceBeam:  1.03%\n","  PlayingViolin:  1.03%\n","  PlayingPiano:  1.03%\n","  PullUps:  1.00%\n","  TennisSwing:  0.83%\n","  ThrowDiscus:  0.83%\n","  TrampolineJumping:  0.83%\n","  Typing:  0.83%\n","  UnevenBars:  0.83%\n","  VolleyballSpiking:  0.83%\n","  TaiChi:  0.83%\n","  TableTennisShot:  0.83%\n","  WalkingWithDog:  0.83%\n","  WallPushups:  0.83%\n","  SkyDiving:  0.83%\n","  Swing:  0.83%\n","  Surfing:  0.83%\n","  Punch:  0.83%\n","  PushUps:  0.83%\n","  Rafting:  0.83%\n","  RockClimbingIndoor:  0.83%\n","  WritingOnBoard:  0.83%\n","  RopeClimbing:  0.83%\n","  Rowing:  0.83%\n","  SalsaSpin:  0.83%\n","  ShavingBeard:  0.83%\n","  Shotput:  0.83%\n","  SkateBoarding:  0.83%\n","  Skiing:  0.83%\n","  Skijet:  0.83%\n","  SoccerJuggling:  0.83%\n","  SoccerPenalty:  0.83%\n","  StillRings:  0.83%\n","  SumoWrestling:  0.83%\n","  YoYo:  0.83%\n"]}],"source":["def prepare_single_video(frames):\n","    frames = frames[None, ...]\n","    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n","    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n","\n","    for i, batch in enumerate(frames):\n","        video_length = batch.shape[0]\n","        length = min(MAX_SEQ_LENGTH, video_length)\n","        for j in range(length):\n","            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n","        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n","\n","    return frame_features, frame_mask\n","\n","\n","def sequence_prediction(path):\n","    class_vocab = label_processor.get_vocabulary()\n","\n","    frames = load_video(os.path.join(\"test\", path))\n","    frame_features, frame_mask = prepare_single_video(frames)\n","    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n","\n","    for i in np.argsort(probabilities)[::-1]:\n","        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n","    return frames\n","\n","\n","\n","test_video = np.random.choice(test[\"video_name\"].values.tolist())\n","print(f\"Test video path: {test_video}\")\n","test_frames = sequence_prediction(test_video)\n"]},{"cell_type":"markdown","metadata":{"id":"a01It9XpY46J"},"source":["__Observations:__\n","- The test video path will be printed.\n","- The predicted class probabilities for the test video will be printed, showing the class label and the corresponding probability.\n","- The frames of the test video will be assigned to the **test_frames** variable."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.1.undefined"}},"nbformat":4,"nbformat_minor":0}
